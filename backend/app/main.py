import contextlib
from contextlib import asynccontextmanager
from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sqlalchemy.ext.asyncio import AsyncSession
from google.genai import types as genai_types

from database import async_engine, AsyncSessionLocal, Base
from models import QueryLog
from llm_service import get_gemini_model
from tool_caller import call_search_web


# ========== Esquemas Pydantic ==========

class QueryRequest(BaseModel):
    """Schema para recibir consultas del usuario"""
    user_name: str
    prompt: str


class QueryResponse(BaseModel):
    """Schema para devolver la respuesta del orquestador"""
    final_answer: str


# ========== Gestor de Vida (Lifespan) ==========

async def init_db():
    """
    Inicializa la base de datos creando todas las tablas definidas en los modelos.
    Se ejecuta al iniciar la aplicaci√≥n.
    """
    async with async_engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    print("‚úÖ Base de datos inicializada correctamente")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Gestor del ciclo de vida de la aplicaci√≥n.
    Se ejecuta al inicio (antes del yield) y al apagado (despu√©s del yield).
    """
    # Startup: Inicializar base de datos
    await init_db()
    yield
    # Shutdown: Aqu√≠ se pueden agregar tareas de limpieza si es necesario
    print("üî¥ Apagando aplicaci√≥n...")


# ========== Instancia de FastAPI ==========

app = FastAPI(
    title="Orquestador de Agente IA",
    description="API para gestionar consultas y coordinar herramientas con n8n",
    version="1.0.0",
    lifespan=lifespan
)


# ========== Middleware de CORS ==========

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En producci√≥n, especificar dominios permitidos
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ========== Dependencia de Base de Datos ==========

async def get_db():
    """
    Dependency para inyectar la sesi√≥n de base de datos en los endpoints.
    Asegura que la sesi√≥n se cierre correctamente despu√©s de cada request.
    """
    db = AsyncSessionLocal()
    try:
        yield db
    finally:
        await db.close()


# ========== Endpoints ==========

@app.get("/")
async def root():
    """Endpoint de verificaci√≥n de estado del servicio"""
    return {
        "status": "Orquestador en l√≠nea",
        "version": "1.0.0",
        "message": "API funcionando correctamente"
    }


@app.post("/api/v1/query", response_model=QueryResponse)
async def process_query(
    request: QueryRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    Endpoint principal para procesar consultas del usuario con agente IA real.
    
    Args:
        request: QueryRequest con user_name y prompt
        db: Sesi√≥n de base de datos (inyectada autom√°ticamente)
    
    Returns:
        QueryResponse con la respuesta final del agente
    
    Flujo del agente:
        1. Recibe la consulta del usuario
        2. Env√≠a el prompt a Gemini con herramientas disponibles
        3. Si Gemini necesita buscar en la web, ejecuta la funci√≥n
        4. Env√≠a los resultados de vuelta a Gemini
        5. Obtiene la respuesta final
        6. Guarda el log en la base de datos
        7. Devuelve la respuesta
    """
    try:
        # Paso 1: Obtener el cliente Gemini configurado
        result = get_gemini_model()
        print(f"üîß Resultado de get_gemini_model: {type(result)}")
        client, tools = result
        print(f"üîß Cliente: {type(client)}, Tools: {type(tools)}")
        
        # Paso 2: Primero preguntar a Gemini si necesita buscar informaci√≥n en la web
        print(f"üì§ Analizando prompt: {request.prompt}")
        analysis_prompt = f"""Analiza la siguiente consulta del usuario y determina si necesitas buscar informaci√≥n actualizada en la web para responderla correctamente.

Consulta del usuario: "{request.prompt}"

Responde √öNICAMENTE con una de estas dos opciones:
- Si necesitas buscar en la web (noticias, eventos actuales, informaci√≥n reciente): Responde solo "BUSCAR: <query optimizada para b√∫squeda>"
- Si puedes responder con tu conocimiento actual: Responde solo "RESPONDER"

Tu respuesta:"""
        
        analysis_response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=analysis_prompt,
        )
        
        analysis_text = analysis_response.text.strip()
        print(f"ü§î An√°lisis de Gemini: {analysis_text}")
        
        final_answer = None
        
        # Paso 3: Verificar si Gemini decidi√≥ buscar en la web
        if analysis_text.startswith("BUSCAR:"):
            # Gemini decidi√≥ usar b√∫squeda web
            query = analysis_text.replace("BUSCAR:", "").strip()
            print(f"üîç Ejecutando b√∫squeda web con query: {query}")
            
            # Paso 4: Llamar a nuestra herramienta real (webhook n8n)
            tool_results_dict = await call_search_web(query=query)
            print(f"‚úÖ Resultados de b√∫squeda obtenidos")
            
            # Paso 5: Enviar los resultados a Gemini para que genere la respuesta final
            final_prompt = f"""El usuario pregunt√≥: "{request.prompt}"

Encontr√© esta informaci√≥n actualizada en la web:
{tool_results_dict}

Por favor, genera una respuesta completa y √∫til para el usuario bas√°ndote en esta informaci√≥n. S√© espec√≠fico y cita datos relevantes."""
            
            print("üì§ Enviando resultados a Gemini para generar respuesta final...")
            final_response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=final_prompt,
            )
            
            final_answer = final_response.text
            print(f"‚úÖ Respuesta final del agente (con b√∫squeda web)")
        else:
            # Gemini decidi√≥ responder directamente
            print("üí¨ Generando respuesta directa sin b√∫squeda web...")
            direct_response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=request.prompt,
            )
            final_answer = direct_response.text
            print(f"‚úÖ Respuesta directa del agente (sin b√∫squeda web)")
        
        # Paso 7: Guardar el log en la base de datos
        db_log = QueryLog(
            user_name=request.user_name,
            prompt_text=request.prompt,
            final_answer=final_answer
        )
        
        db.add(db_log)
        await db.commit()
        await db.refresh(db_log)
        
        print(f"üíæ Log guardado con ID: {db_log.id}")
        
        # Paso 8: Devolver la respuesta al cliente
        return QueryResponse(final_answer=final_answer)
    
    except Exception as e:
        # En caso de error, hacer rollback y devolver error
        await db.rollback()
        print(f"‚ùå Error procesando query: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Error procesando la consulta: {str(e)}"
        )


@app.get("/api/v1/logs")
async def get_logs(db: AsyncSession = Depends(get_db)):
    """
    Endpoint para obtener todos los logs de consultas.
    √ötil para debugging y monitoreo.
    """
    from sqlalchemy import select
    
    try:
        result = await db.execute(select(QueryLog).order_by(QueryLog.timestamp.desc()))
        logs = result.scalars().all()
        
        return {
            "total": len(logs),
            "logs": [
                {
                    "id": log.id,
                    "user_name": log.user_name,
                    "prompt_text": log.prompt_text,
                    "final_answer": log.final_answer,
                    "timestamp": log.timestamp.isoformat()
                }
                for log in logs
            ]
        }
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error obteniendo logs: {str(e)}"
        )
